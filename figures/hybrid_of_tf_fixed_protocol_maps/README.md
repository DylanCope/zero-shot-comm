# About These Results

These results were generated by running the "observation fixing" experiment:
```py
def create_observation_fixing_experiment(epochs=15, **exp_kwargs):
    
    agent = Agent(CHANNEL_SIZE, NUM_CLASSES, first_activation=None)

    play_params = {
        'channel_size': CHANNEL_SIZE,
        'p_mutate': 0.0
    }
    
    return Experiment(
        generate_train_batch, generate_test_batch,
        play_params=play_params, 
        student=agent,
        teacher=agent,
        loss_fn=complete_loss_fn,
        max_epochs=epochs,
        **exp_kwargs
    )

obs_fixing_experiment = create_observation_fixing_experiment()
obs_fixing_experiment.run()
```

With the following training history:

```bash
Running experiment...
Run config:
 {'name': 'experiment', 'max_epochs': 15, 'steps_per_epoch': 50, 'epochs_optimised': 15, 'play_params': {'channel_size': 5, 'p_mutate': 0.0}, 'test_freq': 5, 'test_steps': 25, 'optimiser_config': {'name': 'RMSprop', 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': 100, 'jit_compile': False, 'is_legacy_optimizer': False, 'learning_rate': 0.009999999776482582, 'rho': 0.9, 'momentum': 0.0, 'epsilon': 1e-07, 'centered': False}, 'optimise_agents_separately': False, 'loss_fn': 'complete_loss_fn'}
Epoch 0, Time Taken (mm:ss): 0:10, Mean Loss: 3.847
Test Loss: 4.556, Ground Truth F1-Score: 0.345, Student Error: 1.102, Teacher Error: 0.454, Protocol Diversity: 0.333, Protocol Entropy: 1.6,
Epoch 1, Time Taken (mm:ss): 0:9, Mean Loss: 2.784
Epoch 2, Time Taken (mm:ss): 0:9, Mean Loss: 2.383
Epoch 3, Time Taken (mm:ss): 0:9, Mean Loss: 1.347
Epoch 4, Time Taken (mm:ss): 0:9, Mean Loss: 1.123
Epoch 5, Time Taken (mm:ss): 0:9, Mean Loss: 1.008
Test Loss: 1.002, Ground Truth F1-Score: 1.0, Student Error: 0.0, Teacher Error: 0.001, Protocol Diversity: 1.0, Protocol Entropy: 0.0,
Epoch 6, Time Taken (mm:ss): 0:9, Mean Loss: 1.001
Epoch 7, Time Taken (mm:ss): 0:9, Mean Loss: 1.003
Epoch 8, Time Taken (mm:ss): 0:9, Mean Loss: 1.001
Epoch 9, Time Taken (mm:ss): 0:9, Mean Loss: 1.0
Epoch 10, Time Taken (mm:ss): 0:9, Mean Loss: 1.0
Test Loss: 1.0, Ground Truth F1-Score: 1.0, Student Error: 0.0, Teacher Error: 0.0, Protocol Diversity: 1.0, Protocol Entropy: 0.0,
Epoch 11, Time Taken (mm:ss): 0:9, Mean Loss: 1.0
Epoch 12, Time Taken (mm:ss): 0:9, Mean Loss: 1.0
Epoch 13, Time Taken (mm:ss): 0:9, Mean Loss: 1.0
Epoch 14, Time Taken (mm:ss): 0:9, Mean Loss: 1.0
Test Loss: 1.0, Ground Truth F1-Score: 1.0, Student Error: 0.0, Teacher Error: 0.0, Protocol Diversity: 1.0, Protocol Entropy: 0.0,
Training stopped.
```

However, rather than learning an "observation-fixed" (OF) protocol, the agent learned a "hybrid" protocol where one of the symbols
was fixed to a particular observation. But two other symbols were used equally
for the other two observations, depending on when the symbol appeared.

To investigate more later, the experiment has been saved in the `./experiment` folder using the `export_experiment` function, so that the results can be loaded and analysed later.
