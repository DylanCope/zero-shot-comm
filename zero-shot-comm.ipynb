{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Devices:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.extend(['..'])\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import time\n",
    "import unittest.mock as mock\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "my_devices = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "tf.config.experimental.set_visible_devices(devices=my_devices, device_type='CPU')\n",
    "\n",
    "print('Physical Devices:')\n",
    "for dev in tf.config.list_physical_devices():\n",
    "    print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'teacher_only_loss_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-16f8f30de1b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mzscomm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_game\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mzscomm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mzscomm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_game\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_game\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Projects\\zero-shot-comm\\zscomm\\plot_game.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m from .loss import (\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mstudent_pred_matches_implied_class\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mstudent_pred_matches_test_class\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mteacher_only_loss_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mget_expected_student_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'teacher_only_loss_fn'"
     ]
    }
   ],
   "source": [
    "from zscomm.agent import Agent\n",
    "from zscomm.comm_channel import CommChannel\n",
    "from zscomm.synth_teacher import SyntheticTeacher\n",
    "from zscomm.play_game import *\n",
    "from zscomm.loss import *\n",
    "from zscomm.plot_game import plot_game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that gradients can flow across channel:dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = CommChannel(size=10, temperature=1, one_hot=True)\n",
    "\n",
    "x = tf.Variable([[-1, 2, 4, 0]], dtype=tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = channel(x, training=True)\n",
    "    \n",
    "print(y)\n",
    "tape.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 32\n",
    "use_mnist = False\n",
    "\n",
    "if use_mnist:\n",
    "    class_labels = list(range(NUM_CLASSES))\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    def preprocess_images(images):\n",
    "        images = images / 255.\n",
    "        return np.where(images > .5, 1.0, 0.0).astype('float32')\n",
    "\n",
    "    x_train = preprocess_images(x_train)\n",
    "    train_data = [(x, y) for x, y in zip(x_train, y_train) if y in class_labels]\n",
    "    np.random.shuffle(train_data)\n",
    "\n",
    "    x_test = preprocess_images(x_test)\n",
    "    test_data = [(x, y) for x, y in zip(x_test, y_test) if y in class_labels]\n",
    "    np.random.shuffle(test_data)\n",
    "    \n",
    "else:\n",
    "    class_labels = list(range(1, 1 + NUM_CLASSES))\n",
    "    sample_size = int(np.ceil(np.log2(NUM_CLASSES)))\n",
    "    train_data = [\n",
    "        ([0.0]*(sample_size - len(bin(l)) + 2) + [float(x) for x in bin(l)[2:]], l)\n",
    "        for l in range(1, NUM_CLASSES+1)\n",
    "    ]\n",
    "    test_data = train_data\n",
    "\n",
    "\n",
    "train_data_by_label = {\n",
    "    label: [\n",
    "        img for img, l in train_data\n",
    "        if l == label\n",
    "    ]\n",
    "    for label in class_labels\n",
    "}\n",
    "\n",
    "test_data_by_label = {\n",
    "    label: [\n",
    "        img for img, l in test_data\n",
    "        if l == label\n",
    "    ]\n",
    "    for label in class_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_by_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(source, source_by_label, batch_size=BATCH_SIZE):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    \n",
    "    for batch in range(batch_size):\n",
    "        batch_tars = []\n",
    "        batch_inps = []\n",
    "        \n",
    "        for ts in range(NUM_CLASSES):\n",
    "            sample_space = set(class_labels) - set(batch_tars)\n",
    "            target = random.choice(list(sample_space))\n",
    "            batch_tars.append(target)\n",
    "            inp = random.choice(source_by_label[target])\n",
    "            batch_inps.append(tf.constant(inp))\n",
    "        \n",
    "        target = random.choice(list(set(class_labels)))  \n",
    "        batch_tars.append(target)\n",
    "        inp = random.choice(source_by_label[target])\n",
    "        batch_inps.append(tf.constant(inp))\n",
    "        \n",
    "        batch_tars = [\n",
    "            tf.one_hot(tar-1, NUM_CLASSES)\n",
    "            for tar in batch_tars\n",
    "        ]\n",
    "        \n",
    "        inputs.append(batch_inps)\n",
    "        targets.append(batch_tars)\n",
    "    \n",
    "    inputs = [\n",
    "        tf.concat([[inputs[b][t]] for b in range(batch_size)], 0)\n",
    "        for t in range(NUM_CLASSES + 1)\n",
    "    ]\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    targets = [\n",
    "        tf.concat([[targets[b][t]] for b in range(batch_size)], 0)\n",
    "        for t in range(NUM_CLASSES + 1)\n",
    "    ]\n",
    "    targets = tf.convert_to_tensor(targets)\n",
    "    \n",
    "    return inputs, targets \n",
    "\n",
    "def generate_train_batch():\n",
    "    return generate_batch(train_data, train_data_by_label)\n",
    "\n",
    "def generate_test_batch():\n",
    "    return generate_batch(test_data, test_data_by_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_train_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shape(inputs), tf.shape(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_mnist:\n",
    "    figsize = (10, 10*(1 + len(class_labels)))\n",
    "else:\n",
    "    figsize = (3*(1 + len(class_labels)), 1)\n",
    "\n",
    "select_batch = 0\n",
    "show_rows = 5\n",
    "\n",
    "for row in range(show_rows):\n",
    "    fig, axs = plt.subplots(1, len(class_labels) + 1, \n",
    "                            figsize=figsize)\n",
    "    for ax, inp, tar in zip(axs, inputs, targets):\n",
    "        ax.set_title(tar[row].numpy().argmax() + 1)\n",
    "        if use_mnist:\n",
    "            img = inp[row].numpy()\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            domain = list(range(len(inp[row])))\n",
    "            ax.bar(domain, inp[row])\n",
    "            ax.set_ylim([0, 1])\n",
    "            ax.set_yticks([0, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "comm_channel = CommChannel(size=5, temperature=1, noise=0.5)\n",
    "\n",
    "msg, _ = maybe_mutate_message(comm_channel.get_initial_state(5), \n",
    "                              comm_channel.size, history, 1)\n",
    "history.append({'message_from_teacher': msg})\n",
    "msg, _ = maybe_mutate_message(comm_channel.get_initial_state(5), \n",
    "                              comm_channel.size, history, 1)\n",
    "history.append({'message_from_teacher': msg})\n",
    "msg, _ = maybe_mutate_message(comm_channel.get_initial_state(5), \n",
    "                              comm_channel.size, history, 1)\n",
    "history.append({'message_from_teacher': msg})\n",
    "msg, _ = maybe_mutate_message(comm_channel.get_initial_state(5), \n",
    "                              comm_channel.size, history, 1)\n",
    "history.append({'message_from_teacher': msg})\n",
    "tf.transpose([\n",
    "    tf.argmax(item['message_from_teacher'], axis=-1) \n",
    "    for item in history\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_mnist:\n",
    "    encoder_latent_dim = 64\n",
    "    _, *img_shape = x_train.shape\n",
    "    encoder = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=img_shape),\n",
    "            tf.keras.layers.Reshape(target_shape=(*img_shape, 1)),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            # No activation\n",
    "            tf.keras.layers.Dense(encoder_latent_dim),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "else: \n",
    "    encoder = None\n",
    "\n",
    "CHANNEL_SIZE = 5\n",
    "\n",
    "agent_1 = Agent(CHANNEL_SIZE, NUM_CLASSES, encoder=encoder)\n",
    "agent_2 = Agent(CHANNEL_SIZE, NUM_CLASSES, encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, _ = generate_train_batch()\n",
    "preds, history = play_game(inputs, agent_1, agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_train_batch()\n",
    "\n",
    "teacher = agent_1\n",
    "student = agent_2\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    outputs = play_game(inputs, teacher, student, \n",
    "                        training=True, \n",
    "                        p_mutate=0.5)\n",
    "\n",
    "    loss = combined_loss_function(outputs, targets)\n",
    "\n",
    "teacher_grads = tape.gradient(loss, teacher.trainable_variables)\n",
    "student_grads = tape.gradient(loss, teacher.trainable_variables)\n",
    "\n",
    "for v, g in zip(agent_1.trainable_variables, teacher_grads):\n",
    "    print(f'{v.name} teacher grad norm: {tf.reduce_sum(g**2)**0.5}')\n",
    "\n",
    "print()\n",
    "\n",
    "for v, g in zip(agent_1.trainable_variables, student_grads):\n",
    "    print(f'{v.name} student grad norm: {tf.reduce_sum(g**2)**0.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_train_batch()\n",
    "outputs = play_game(inputs, agent_1, agent_2,\n",
    "                    p_mutate=0.5, \n",
    "                    training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_game(inputs, outputs, targets, select_batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_train_batch()\n",
    "\n",
    "teacher = agent_1\n",
    "student = mock.MagicMock(return_value=(None, None, None))\n",
    "\n",
    "def compute_loss():\n",
    "    outputs = play_game(inputs, teacher, student, \n",
    "                        training=True, \n",
    "                        p_mutate=0)\n",
    "\n",
    "    return protocol_diversity_loss(outputs)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    loss = compute_loss() \n",
    "\n",
    "teacher_grads = tape.gradient(loss, teacher.trainable_variables)\n",
    "\n",
    "for v, g in zip(agent_1.trainable_variables, teacher_grads):\n",
    "    print(f'{v.name} teacher grad norm: {tf.reduce_sum(g**2)**0.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser.apply_gradients(zip(teacher_grads, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_student_training_step(optimiser, data, agent):\n",
    "    inputs, targets = data\n",
    "    teacher = SyntheticTeacher(CHANNEL_SIZE, NUM_CLASSES, targets)\n",
    "    student = agent\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        outputs = play_game(inputs, teacher, student, \n",
    "                            training=True, \n",
    "                            stop_gradients_on_all_comm=True,\n",
    "                            p_mutate=0)\n",
    "    \n",
    "        loss = student_pred_matches_implied_class(outputs, targets)\n",
    "    \n",
    "    grads = tape.gradient(loss, agent.trainable_variables)\n",
    "    optimiser.apply_gradients(zip(grads, agent.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def only_teacher_test_msg_training_step(optimiser, data, agent):\n",
    "    inputs, targets = data\n",
    "    teacher = agent\n",
    "    student = mock.MagicMock(return_value=(None, None, None))\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        outputs = play_game(inputs, teacher, student, \n",
    "                            training=True, \n",
    "                            stop_gradients_on_all_comm=True,\n",
    "                            p_mutate=1)\n",
    "    \n",
    "        loss = teacher_test_message_is_correct(outputs, targets)\n",
    "    \n",
    "    grads = tape.gradient(loss, agent.trainable_variables)\n",
    "    optimiser.apply_gradients(zip(grads, agent.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def only_teacher_create_protocol_training_step(optimiser, data, agent):\n",
    "    inputs, targets = data\n",
    "    teacher = agent\n",
    "    student = mock.MagicMock(return_value=(None, None, None))\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        outputs = play_game(inputs, teacher, student, \n",
    "                            training=True, \n",
    "                            stop_gradients_on_all_comm=False,\n",
    "                            p_mutate=0)\n",
    "    \n",
    "#         loss = teacher_test_message_is_correct(outputs, targets)\n",
    "        loss = protocol_diversity_loss(outputs)\n",
    "    \n",
    "    grads = tape.gradient(loss, agent.trainable_variables)\n",
    "    optimiser.apply_gradients(zip(grads, agent.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def self_play_training_step(optimiser, data, agent):\n",
    "    inputs, targets = data\n",
    "    teacher = student = agent\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        outputs = play_game(inputs, teacher, student, \n",
    "                            training=True, \n",
    "                            p_mutate=1)\n",
    "    \n",
    "        loss = combined_loss_fn(outputs, targets)\n",
    "    \n",
    "    grads = tape.gradient(loss, agent.trainable_variables)\n",
    "    optimiser.apply_gradients(zip(grads, agent.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def complete_loss_fn(outputs, targets):\n",
    "    loss = teacher_test_message_is_correct(outputs, targets)\n",
    "    loss = loss + protocol_diversity_loss(outputs)\n",
    "    loss = loss + student_pred_matches_implied_class(outputs, targets)\n",
    "    return loss\n",
    "\n",
    "def other_play_training_step(optimiser, data, teacher, student):\n",
    "    \n",
    "    inputs, targets = data\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        outputs = play_game(inputs, teacher, student, \n",
    "                            training=True, \n",
    "                            p_mutate=1)\n",
    "    \n",
    "        loss = complete_loss_fn(outputs, targets)\n",
    "    \n",
    "    trainable_vars = teacher.trainable_variables + student.trainable_variables\n",
    "    grads = tape.gradient(loss, trainable_vars)\n",
    "    optimiser.apply_gradients(zip(grads, trainable_vars))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        training_step_fn, \n",
    "        *agents,\n",
    "        max_epochs=20, \n",
    "        steps_per_epoch=100, \n",
    "        step_print_freq=5,\n",
    "        lr=1e-2, # learning rate\n",
    "    ):\n",
    "        self.training_step_fn = training_step_fn\n",
    "        self.agents = agents\n",
    "        \n",
    "        self.max_epochs = max_epochs\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.step_print_freq = step_print_freq\n",
    "        \n",
    "        self.training_history = []\n",
    "        self.epoch = 0\n",
    "        self.optimiser = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'max_epochs': self.max_epochs,\n",
    "            'steps_per_epoch': self.steps_per_epoch,\n",
    "            'epochs_optimised': self.epoch,\n",
    "            'training_step_fn': str(self.training_step_fn),\n",
    "            'optimiser_config': self.optimiser.get_config(),\n",
    "        }\n",
    "    \n",
    "    def print_history(self):\n",
    "        for e, item in enumerate(self.training_history):\n",
    "            mins = int(item['seconds_taken']) // 60\n",
    "            secs = int(item['seconds_taken']) % 60\n",
    "            t = f'{mins}:{secs}'\n",
    "            l = int(10000 * item['loss'].numpy().mean()) / 10000\n",
    "            print(f'Epoch {e}, Time Taken (mm:ss): {t}, Loss: {l}')\n",
    "\n",
    "    def print_step_progress(self, step, step_mean_loss):\n",
    "        l = int(10000 * step_mean_loss.numpy().mean()) / 10000\n",
    "        p = int(10000 * step / self.steps_per_epoch) / 100\n",
    "        print(f'Epoch {self.epoch}, {p}% complete, Loss: {l}')\n",
    "\n",
    "    def training_step(self):\n",
    "        data = generate_train_batch()\n",
    "        return self.training_step_fn(self.optimiser, data, *self.agents)\n",
    "    \n",
    "    def train(self):\n",
    "        self.print_history()\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            while self.epoch < self.max_epochs:\n",
    "                mean_loss = tf.zeros((1,))\n",
    "\n",
    "                start_time = time.time()\n",
    "                for step in range(self.steps_per_epoch):\n",
    "\n",
    "                    loss = self.training_step()\n",
    "                    mean_loss = (mean_loss + tf.reduce_mean(loss)) / 2.0\n",
    "\n",
    "                    if step % self.step_print_freq == 0:\n",
    "                        self.print_history()\n",
    "                        self.print_step_progress(step, mean_loss)\n",
    "                        clear_output(wait=True)\n",
    "\n",
    "                seconds_taken = time.time() - start_time\n",
    "                self.training_history.append({\n",
    "                    'loss': mean_loss, \n",
    "                    'seconds_taken': seconds_taken\n",
    "                })\n",
    "\n",
    "                self.epoch += 1\n",
    "                self.print_history()\n",
    "                clear_output(wait=True)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "        self.print_history()\n",
    "        print('Training stopped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = Agent(CHANNEL_SIZE, NUM_CLASSES, encoder=encoder)\n",
    "agent_2 = Agent(CHANNEL_SIZE, NUM_CLASSES, encoder=encoder)\n",
    "\n",
    "experiment = Experiment(other_play_training_step, agent_1, agent_2,\n",
    "                        max_epochs=50)\n",
    "experiment = Experiment(only_teacher_test_msg_training_step, agent_1)\n",
    "# experiment = Experiment(only_teacher_create_protocol_training_step, agent_1)\n",
    "# experiment = Experiment(only_student_training_step, agent_1)\n",
    "\n",
    "print(experiment.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Training Loss History')\n",
    "sns.lineplot(x=range(len(experiment.training_history)), \n",
    "             y=[item['loss'].numpy()[0] for item in experiment.training_history]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Model Graph in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up logging.\n",
    "# from datetime import datetime\n",
    "# stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# logdir = f'logs\\\\{stamp}'\n",
    "# writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "# # Bracket the function call with\n",
    "# # tf.summary.trace_on() and tf.summary.trace_export().\n",
    "# tf.summary.trace_off()\n",
    "# tf.summary.trace_on(graph=True, profiler=True)\n",
    "# # Call only one tf.function when tracing.\n",
    "\n",
    "# @tf.function\n",
    "# def graph_training_step():\n",
    "#     return only_teacher_training_step(agent_1)\n",
    "\n",
    "\n",
    "# graph_training_step()\n",
    "\n",
    "# with writer.as_default():\n",
    "#     tf.summary.trace_export(\n",
    "#         name=\"teacher_only_training_step\",\n",
    "#         step=0,\n",
    "#         profiler_outdir=logdir)\n",
    "    \n",
    "# tf.summary.trace_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_student_test_play(inputs, targets):\n",
    "    teacher = SyntheticTeacher(CHANNEL_SIZE, NUM_CLASSES, targets)\n",
    "    student = agent_1\n",
    "    return play_game(inputs, teacher, student, \n",
    "                     training=False,\n",
    "                     p_mutate=0)\n",
    "\n",
    "def only_teacher_test_play_use_protocol(inputs, targets):\n",
    "    student = mock.MagicMock(return_value=(None, None, None))\n",
    "    teacher = agent_1\n",
    "    return play_game(inputs, teacher, student, \n",
    "                     training=False,\n",
    "                     p_mutate=1)\n",
    "\n",
    "def only_teacher_test_play_create_protocol(inputs, targets):\n",
    "    student = mock.MagicMock(return_value=(None, None, None))\n",
    "    teacher = agent_1\n",
    "    return play_game(inputs, teacher, student, \n",
    "                     training=False,\n",
    "                     p_mutate=0)\n",
    "\n",
    "def self_play_test(inputs, targets):\n",
    "    return play_game(inputs, agent_1, agent_1, \n",
    "                     training=False,\n",
    "                     p_mutate=1)\n",
    "\n",
    "def other_play_test(inputs, targets):\n",
    "    return play_game(inputs, agent_1, agent_2, \n",
    "                     training=False,\n",
    "                     p_mutate=1)\n",
    "\n",
    "# test_play = only_student_test_play\n",
    "# test_loss_fn = student_pred_matches_implied_class\n",
    "\n",
    "test_play = only_teacher_test_play_create_protocol\n",
    "test_loss_fn = lambda o, t: protocol_diversity_loss(o)\n",
    "\n",
    "# test_play = other_play_test\n",
    "# test_loss_fn = complete_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_games = 50 # in batches\n",
    "\n",
    "test_samples = [generate_test_batch() for _ in range(num_test_games)]\n",
    "\n",
    "games_played = [\n",
    "    (x, y, *test_play(x, y))\n",
    "    for x, y in test_samples\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    inputs, targets, *outputs = games_played[5+i]\n",
    "    plot_game(inputs, outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = tf.zeros((1,))\n",
    "for inputs, targets, *outputs in games_played:\n",
    "    loss = test_loss_fn(outputs, targets)\n",
    "    test_loss = (test_loss + loss) / 2\n",
    "    \n",
    "print(tf.reduce_mean(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.concat([\n",
    "    tf.argmax(y[-1], axis=-1) \n",
    "    for _, y in test_samples\n",
    "], axis=0)\n",
    "\n",
    "preds = tf.concat([\n",
    "    tf.argmax(student_preds, axis=-1) \n",
    "    for _, _, student_preds, _ in games_played\n",
    "], axis=0)\n",
    "\n",
    "conf_matrix = tf.math.confusion_matrix(labels, preds)\n",
    "\n",
    "col_totals = tf.reduce_sum(conf_matrix, axis=0)\n",
    "col_totals = tf.repeat(col_totals, tf.shape(conf_matrix)[0])\n",
    "col_totals = tf.reshape(col_totals, tf.shape(conf_matrix))\n",
    "col_totals = tf.transpose(col_totals)\n",
    "\n",
    "conf_matrix = (conf_matrix / col_totals).numpy()\n",
    "conf_matrix[np.where(np.isnan(conf_matrix))] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(conf_matrix, annot=True, vmin=0, vmax=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows correspond to the true labels and the columns to the predicted labels. Each column is divided by its sum in order to show the percentage of the time the model predicts the given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = tf.concat([\n",
    "    history[-1]['message_from_teacher'] \n",
    "    for *_, history in games_played\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lm_map(label, message):\n",
    "    *_, msg_size = tf.shape(message)\n",
    "    lm_map_shape = (NUM_CLASSES, msg_size)\n",
    "    class_indices = [i for i, _ in enumerate(class_labels)]\n",
    "    indices = tf.reshape(tf.repeat(class_indices, msg_size), \n",
    "                         lm_map_shape)\n",
    "    indices = tf.cast(indices, tf.int64) == label\n",
    "    return message * tf.cast(indices, tf.float32)\n",
    "    \n",
    "*_, msg_size = tf.shape(messages)\n",
    "lm_map = tf.zeros((NUM_CLASSES, msg_size))\n",
    "for label, message in zip(labels, messages):\n",
    "    lm_map = lm_map + make_lm_map(label, message)\n",
    "\n",
    "row_totals = tf.reduce_sum(lm_map, axis=1)\n",
    "row_totals = tf.repeat(row_totals, msg_size)\n",
    "row_totals = tf.reshape(row_totals, tf.shape(lm_map))\n",
    "\n",
    "lm_map = lm_map / row_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(lm_map, vmin=0, vmax=1);\n",
    "plt.ylabel('Class')\n",
    "plt.xlabel('Symbol')\n",
    "plt.title('Communication Protocol')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_file = f'models/test/{int(time.time())}'\n",
    "# weights_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
