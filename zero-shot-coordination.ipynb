{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Devices:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "print('Physical Devices:')\n",
    "for dev in tf.config.list_physical_devices():\n",
    "    print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zscomm.agent import Agent\n",
    "from zscomm.comm_channel import CommChannel\n",
    "from zscomm.synth_teacher import SyntheticTeacher\n",
    "from zscomm.data import *\n",
    "from zscomm.play_game import *\n",
    "from zscomm.loss import *\n",
    "from zscomm.experiment import Experiment\n",
    "from zscomm.plot_game import plot_game\n",
    "from zscomm.analysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 32\n",
    "CHANNEL_SIZE = 5\n",
    "USE_MNIST = False\n",
    "\n",
    "if USE_MNIST:\n",
    "    TRAIN_DATA, TEST_DATA = get_mnist_data(num_classes=NUM_CLASSES)\n",
    "else:\n",
    "    TRAIN_DATA, TEST_DATA = get_simple_card_data(num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_batch():\n",
    "    return generate_batch(TRAIN_DATA,\n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          num_classes=NUM_CLASSES)\n",
    "\n",
    "\n",
    "def generate_test_batch():\n",
    "    return generate_batch(TEST_DATA,\n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_other_play_experiment(p_mutate=0.3):\n",
    "    \n",
    "    student = Agent(CHANNEL_SIZE, NUM_CLASSES)\n",
    "    teacher = Agent(CHANNEL_SIZE, NUM_CLASSES)\n",
    "\n",
    "    play_params =  {'p_mutate': p_mutate}\n",
    "    \n",
    "    return Experiment(\n",
    "        generate_train_batch, generate_test_batch,\n",
    "        play_params=play_params, \n",
    "        student=student,\n",
    "        teacher=teacher,\n",
    "        loss_fn=complete_loss_fn\n",
    "    )\n",
    "\n",
    "def create_other_play_separate_optimise_experiment(p_mutate=0.3):\n",
    "    \n",
    "    student = Agent(CHANNEL_SIZE, NUM_CLASSES)\n",
    "    teacher = Agent(CHANNEL_SIZE, NUM_CLASSES)\n",
    "\n",
    "    play_params =  {'p_mutate': p_mutate}\n",
    "    \n",
    "    return Experiment(\n",
    "        generate_train_batch, generate_test_batch,\n",
    "        play_params=play_params, \n",
    "        student=student,\n",
    "        teacher=teacher,\n",
    "        student_loss_fn=student_pred_matches_implied_class,\n",
    "        teacher_loss_fn=student_pred_matches_test_class\n",
    "    )\n",
    "\n",
    "def create_self_play_experiment(p_mutate=0.3, **exp_kwargs):\n",
    "    \n",
    "    agent = Agent(CHANNEL_SIZE, NUM_CLASSES)\n",
    "\n",
    "    play_params =  {'p_mutate': p_mutate}\n",
    "    \n",
    "    return Experiment(\n",
    "        generate_train_batch, generate_test_batch,\n",
    "        play_params=play_params, \n",
    "        student=agent,\n",
    "        teacher=agent,\n",
    "        loss_fn=complete_loss_fn,\n",
    "        **exp_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaExperiment:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        p_mutate,  \n",
    "        num_experiments=3,\n",
    "        create_experiment_fn=create_self_play_experiment,\n",
    "        **make_experiment_kwargs\n",
    "    ):\n",
    "        self.p_mutate = p_mutate\n",
    "        self.num_experiments = num_experiments\n",
    "        self.experiments = [\n",
    "            {\n",
    "                'experiment': create_experiment_fn(\n",
    "                    p_mutate, **make_experiment_kwargs\n",
    "                ),\n",
    "                'status': 'Not Run',\n",
    "                'results': None,\n",
    "                'index': i,\n",
    "            }\n",
    "            for i in range(num_experiments)\n",
    "        ]\n",
    "        for item in self.experiments:\n",
    "            item['experiment'].print_my_history = item['experiment'].print_history\n",
    "            item['experiment'].print_history = self.print_history\n",
    "        \n",
    "    def print_history(self):\n",
    "        num_complete = len([\n",
    "            item for item in self.experiments\n",
    "            if item['status'] == 'Complete'\n",
    "        ])\n",
    "        for item in self.experiments:\n",
    "            if item['status'] == 'Complete':\n",
    "                print(f\"Results of experiment {item['index']}:\")\n",
    "                item['experiment'].print_test_metrics(item['results'])\n",
    "        for item in self.experiments:\n",
    "            if item['status'] == 'In Progress':\n",
    "                print(f\"Running experiment {item['index']}\", \n",
    "                      f'({num_complete}/{self.num_experiments} complete):')\n",
    "                item['experiment'].print_my_history()\n",
    "                break\n",
    "\n",
    "    def is_finished(self):\n",
    "        return all([\n",
    "            item['status'] == 'Complete' \n",
    "            for item in self.experiments\n",
    "        ])\n",
    "    \n",
    "    def get_experiment_to_run(self):\n",
    "        for item in self.experiments:\n",
    "            if item['status'] == 'In Progress':\n",
    "                return item\n",
    "        not_run = [\n",
    "            item\n",
    "            for item in self.experiments\n",
    "            if item['status'] == 'Not Run'\n",
    "        ]\n",
    "        if len(not_run) == 0:\n",
    "            return None\n",
    "        return not_run[0]\n",
    "    \n",
    "    def get_experiment_results(self, experiment):\n",
    "        test_metrics_items = [\n",
    "            item['test_metrics']\n",
    "            for item in experiment.training_history\n",
    "            if 'test_metrics' in item\n",
    "        ]\n",
    "        return test_metrics_items[-1]\n",
    "    \n",
    "    def run(self):\n",
    "        try:\n",
    "            while not self.is_finished():\n",
    "                experiment_item = self.get_experiment_to_run()\n",
    "                index = experiment_item['index']\n",
    "                experiment = experiment_item['experiment']\n",
    "                \n",
    "                self.experiments[index]['status'] = 'In Progress'\n",
    "                \n",
    "                experiment.train(catch_interrupt=False)\n",
    "                \n",
    "                self.experiments[index]['results'] = \\\n",
    "                    self.get_experiment_results(experiment)\n",
    "                self.experiments[index]['status'] = 'Complete'\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        \n",
    "        clear_output()\n",
    "        self.print_history()\n",
    "        print('Run Stopped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_experiment = MetaExperiment(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of experiment 0:\n",
      "Test Loss: 4.935, Ground Truth F1-Score: 0.356, Student Error: 1.416, Teacher Error: 1.326, Protocol Diversity: 0.53,\n",
      "Results of experiment 1:\n",
      "Test Loss: 4.688, Ground Truth F1-Score: 0.339, Student Error: 1.213, Teacher Error: 1.367, Protocol Diversity: 0.546,\n",
      "Results of experiment 2:\n",
      "Test Loss: 4.643, Ground Truth F1-Score: 0.311, Student Error: 1.239, Teacher Error: 1.22, Protocol Diversity: 0.526,\n",
      "Run Stopped.\n"
     ]
    }
   ],
   "source": [
    "meta_experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Model Graph in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up logging.\n",
    "# from datetime import datetime\n",
    "# import tensorboard\n",
    "# %load_ext tensorboard\n",
    "\n",
    "# stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# logdir = f'logs\\\\{stamp}'\n",
    "# writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "# # Bracket the function call with\n",
    "# # tf.summary.trace_on() and tf.summary.trace_export().\n",
    "# tf.summary.trace_off()\n",
    "# tf.summary.trace_on(graph=True, profiler=True)\n",
    "# # Call only one tf.function when tracing.\n",
    "\n",
    "# @tf.function\n",
    "# def graph_training_step():\n",
    "#     return only_teacher_training_step(agent_1)\n",
    "\n",
    "# graph_training_step()\n",
    "\n",
    "# with writer.as_default():\n",
    "#     tf.summary.trace_export(\n",
    "#         name=\"teacher_only_training_step\",\n",
    "#         step=0,\n",
    "#         profiler_outdir=logdir)\n",
    "    \n",
    "# tf.summary.trace_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_played, test_metrics = experiment.run_tests()\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_played, _ = experiment.run_tests()\n",
    "conf_matrix = compute_confusion_matrix(games_played)\n",
    "sns.heatmap(conf_matrix, annot=True, vmin=0, vmax=1)\n",
    "plt.title('Ground Truth Confusion Matrix')\n",
    "plt.ylabel('Predicted Class')\n",
    "plt.xlabel('Actual Class');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_played, _ = experiment2.run_tests()\n",
    "conf_matrix = compute_confusion_matrix(games_played)\n",
    "sns.heatmap(conf_matrix, annot=True, vmin=0, vmax=1)\n",
    "plt.title('Ground Truth Confusion Matrix')\n",
    "plt.ylabel('Predicted Class')\n",
    "plt.xlabel('Actual Class');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows correspond to the true labels and the columns to the predicted labels. Each column is divided by its sum in order to show the percentage of the time the model predicts the given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_played, _ = experiment.run_tests()\n",
    "mean_class_message_map = create_mean_class_message_map(games_played)\n",
    "sns.heatmap(mean_class_message_map, vmin=0, vmax=1);\n",
    "plt.ylabel('Class')\n",
    "plt.xlabel('Symbol')\n",
    "plt.title('Communication Protocol')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_played, _ = experiment2.run_tests()\n",
    "mean_class_message_map = create_mean_class_message_map(games_played)\n",
    "sns.heatmap(mean_class_message_map, vmin=0, vmax=1);\n",
    "plt.ylabel('Class')\n",
    "plt.xlabel('Symbol')\n",
    "plt.title('Communication Protocol')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_test_batch()\n",
    "outputs = play_game(\n",
    "    inputs, \n",
    "    experiment.teacher, \n",
    "    experiment2.student, \n",
    "    training=False, \n",
    "    p_mutate=0,\n",
    ")\n",
    "\n",
    "teacher_error, protocol_diversity = experiment.get_teacher_test_metrics([(inputs, targets, outputs)])\n",
    "f1, student_error = experiment.get_student_test_metrics([(inputs, targets, outputs)])\n",
    "loss = experiment.get_test_loss([(inputs, targets, outputs)])\n",
    "print(f'Teacher Error: {teacher_error}, Protocol Diversity {protocol_diversity}, Student Error: {student_error}, F1 Score {f1}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_game(inputs, outputs, targets, select_batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_test_batch()\n",
    "outputs = play_game(\n",
    "    inputs, \n",
    "    experiment2.teacher, \n",
    "    experiment.student, \n",
    "    training=False, \n",
    "    p_mutate=0,\n",
    ")\n",
    "\n",
    "teacher_error, protocol_diversity = experiment.get_teacher_test_metrics([(inputs, targets, outputs)])\n",
    "f1, student_error = experiment.get_student_test_metrics([(inputs, targets, outputs)])\n",
    "loss = experiment.get_test_loss([(inputs, targets, outputs)])\n",
    "print(f'Teacher Error: {teacher_error}, Protocol Diversity {protocol_diversity}, Student Error: {student_error}, F1 Score {f1}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_game(inputs, outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
