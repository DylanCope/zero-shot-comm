{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Devices:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "print('Physical Devices:')\n",
    "for dev in tf.config.list_physical_devices():\n",
    "    print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zscomm.agent import Agent\n",
    "from zscomm.comm_channel import CommChannel\n",
    "from zscomm.synth_teacher import SyntheticTeacher\n",
    "from zscomm.data import *\n",
    "from zscomm.play_game import *\n",
    "from zscomm.loss import *\n",
    "from zscomm.experiment import Experiment\n",
    "from zscomm.plot_game import plot_game\n",
    "from zscomm.analysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 32\n",
    "CHANNEL_SIZE = 5\n",
    "USE_MNIST = False\n",
    "\n",
    "if USE_MNIST:\n",
    "    TRAIN_DATA, TEST_DATA = get_mnist_data(num_classes=NUM_CLASSES)\n",
    "else:\n",
    "    TRAIN_DATA, TEST_DATA = get_simple_card_data(num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_batch():\n",
    "    return generate_batch(TRAIN_DATA,\n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          num_classes=NUM_CLASSES)\n",
    "\n",
    "\n",
    "def generate_test_batch():\n",
    "    return generate_batch(TEST_DATA,\n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_other_play_experiment(p_mutate=0.3):\n",
    "    \n",
    "    student = Agent(CHANNEL_SIZE, NUM_CLASSES)\n",
    "    teacher = Agent(CHANNEL_SIZE, NUM_CLASSES)\n",
    "\n",
    "    play_params =  {'p_mutate': p_mutate}\n",
    "    \n",
    "    return Experiment(\n",
    "        generate_train_batch, generate_test_batch,\n",
    "        play_params=play_params, \n",
    "        student=student,\n",
    "        teacher=teacher,\n",
    "        loss_fn=complete_loss_fn\n",
    "    )\n",
    "\n",
    "def create_other_play_separate_optimise_experiment(p_mutate=0.3):\n",
    "    \n",
    "    student = Agent(CHANNEL_SIZE, NUM_CLASSES)\n",
    "    teacher = Agent(CHANNEL_SIZE, NUM_CLASSES)\n",
    "\n",
    "    play_params =  {'p_mutate': p_mutate}\n",
    "    \n",
    "    return Experiment(\n",
    "        generate_train_batch, generate_test_batch,\n",
    "        play_params=play_params, \n",
    "        student=student,\n",
    "        teacher=teacher,\n",
    "        student_loss_fn=student_pred_matches_implied_class,\n",
    "        teacher_loss_fn=student_pred_matches_test_class\n",
    "    )\n",
    "\n",
    "def create_self_play_experiment(p_mutate=0.3, **exp_kwargs):\n",
    "    \n",
    "    agent = Agent(CHANNEL_SIZE, NUM_CLASSES)\n",
    "\n",
    "    play_params =  {'p_mutate': p_mutate}\n",
    "    \n",
    "    return Experiment(\n",
    "        generate_train_batch, generate_test_batch,\n",
    "        play_params=play_params, \n",
    "        student=agent,\n",
    "        teacher=agent,\n",
    "        loss_fn=complete_loss_fn,\n",
    "        **exp_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaExperiment:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        p_mutate=0.5,  \n",
    "        num_experiments=3,\n",
    "        create_experiment_fn=create_self_play_experiment,\n",
    "        **make_experiment_kwargs\n",
    "    ):\n",
    "        self.p_mutate = p_mutate\n",
    "        self.num_experiments = num_experiments\n",
    "        self.experiments = [\n",
    "            {\n",
    "                'experiment': create_experiment_fn(\n",
    "                    p_mutate, **make_experiment_kwargs\n",
    "                ),\n",
    "                'status': 'Not Run',\n",
    "                'results': None,\n",
    "                'index': i,\n",
    "            }\n",
    "            for i in range(num_experiments)\n",
    "        ]\n",
    "        for item in self.experiments:\n",
    "            item['experiment'].print_my_history = item['experiment'].print_history\n",
    "            item['experiment'].print_history = self.print_history\n",
    "        \n",
    "    def print_history(self):\n",
    "        num_complete = len([\n",
    "            item for item in self.experiments\n",
    "            if item['status'] == 'Complete'\n",
    "        ])\n",
    "        for item in self.experiments:\n",
    "            if item['status'] == 'Complete':\n",
    "                print(f\"Results of experiment {item['index']}:\")\n",
    "                item['experiment'].print_test_metrics(item['results'])\n",
    "        for item in self.experiments:\n",
    "            if item['status'] == 'In Progress':\n",
    "                print(f\"Running experiment {item['index']}\", \n",
    "                      f'({num_complete}/{self.num_experiments} complete):')\n",
    "                item['experiment'].print_my_history()\n",
    "                break\n",
    "\n",
    "    def is_finished(self):\n",
    "        return all([\n",
    "            item['status'] == 'Complete' \n",
    "            for item in self.experiments\n",
    "        ])\n",
    "    \n",
    "    def get_experiment_to_run(self):\n",
    "        for item in self.experiments:\n",
    "            if item['status'] == 'In Progress':\n",
    "                return item\n",
    "        not_run = [\n",
    "            item\n",
    "            for item in self.experiments\n",
    "            if item['status'] == 'Not Run'\n",
    "        ]\n",
    "        if len(not_run) == 0:\n",
    "            return None\n",
    "        return not_run[0]\n",
    "    \n",
    "    def get_experiment_results(self, experiment):\n",
    "        test_metrics_items = [\n",
    "            item['test_metrics']\n",
    "            for item in experiment.training_history\n",
    "            if 'test_metrics' in item\n",
    "        ]\n",
    "        return test_metrics_items[-1]\n",
    "    \n",
    "    def _run_internal(self):\n",
    "        while not self.is_finished():\n",
    "            experiment_item = self.get_experiment_to_run()\n",
    "            index = experiment_item['index']\n",
    "            experiment = experiment_item['experiment']\n",
    "\n",
    "            self.experiments[index]['status'] = 'In Progress'\n",
    "\n",
    "            experiment.run(catch_interrupt=False)\n",
    "\n",
    "            self.experiments[index]['results'] = \\\n",
    "                self.get_experiment_results(experiment)\n",
    "            self.experiments[index]['status'] = 'Complete'\n",
    "    \n",
    "    def run(self, catch_interrupt=True):\n",
    "        if catch_interrupt:\n",
    "            try:\n",
    "                self._run_internal()\n",
    "            except KeyboardInterrupt:\n",
    "                pass\n",
    "            self._run_internal()\n",
    "        else:\n",
    "            self._run_internal()\n",
    "        \n",
    "        clear_output()\n",
    "        self.print_history()\n",
    "        print('Run Stopped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_experiment = MetaExperiment(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of experiment 0:\n",
      "Test Loss: 1.137, Ground Truth F1-Score: 0.961, Student Error: 0.069, Teacher Error: 0.031, Protocol Diversity: 0.982,\n",
      "Results of experiment 1:\n",
      "Test Loss: 1.175, Ground Truth F1-Score: 0.96, Student Error: 0.073, Teacher Error: 0.023, Protocol Diversity: 0.961,\n",
      "Results of experiment 2:\n",
      "Test Loss: 1.19, Ground Truth F1-Score: 0.953, Student Error: 0.078, Teacher Error: 0.003, Protocol Diversity: 0.946,\n",
      "Run Stopped.\n"
     ]
    }
   ],
   "source": [
    "meta_experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def test_game(teacher, student, num_tests=5):\n",
    "    games_played = []\n",
    "    for _ in range(num_tests):\n",
    "        inputs, targets = generate_test_batch()\n",
    "        outputs = play_game(\n",
    "            inputs, teacher, student, \n",
    "            training=False, \n",
    "            p_mutate=0,\n",
    "        )\n",
    "        games_played.append((inputs, targets, outputs))\n",
    "    return games_played\n",
    "\n",
    "\n",
    "def measure_zero_shot_coordination(meta_experiment):\n",
    "    results = []\n",
    "    \n",
    "    for item_1, item_2 in combinations(meta_experiment.experiments, 2):\n",
    "        e1 = item_1['experiment']\n",
    "        e2 = item_2['experiment']\n",
    "        \n",
    "        games_played = test_game(e1.teacher, e2.student)\n",
    "        test_metrics = e1.extract_test_metrics(games_played)\n",
    "        results.append(test_metrics['mean_ground_truth_f1'])\n",
    "        \n",
    "        games_played = test_game(e2.teacher, e1.student)\n",
    "        test_metrics = e1.extract_test_metrics(games_played)\n",
    "        results.append(test_metrics['mean_ground_truth_f1'])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9500000000000001, 1.0, 0.65, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_zero_shot_coordination(meta_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VaryPMutateExperiment(MetaExperiment):\n",
    "    \n",
    "    def __init__(self, num_intervals=5, **experiment_kwargs):\n",
    "        self.num_experiments = num_intervals\n",
    "        self.experiments = [\n",
    "            {\n",
    "                'experiment': MetaExperiment(\n",
    "                    p_mutate=i/num_intervals, **experiment_kwargs\n",
    "                ),\n",
    "                'status': 'Not Run',\n",
    "                'results': None,\n",
    "                'index': i,\n",
    "            }\n",
    "            for i in range(num_intervals+1)\n",
    "        ]\n",
    "        for item_2 in self.experiments:\n",
    "            for item_2 in item_1['experiment'].experiments:\n",
    "                item_2['experiment'].print_my_history = item_2['experiment'].print_history\n",
    "                item_2['experiment'].print_history = self.print_history\n",
    "            item['experiment'].print_test_metrics = lambda s: self.print_test_metrics(s)\n",
    "            \n",
    "    def print_test_metrics(self, experiment):\n",
    "        for item in self.experiments:\n",
    "            if item['experiment'] == experiment:\n",
    "                print('Zero-shot coordination F1-Scores:', item['results'])\n",
    "                return\n",
    "    \n",
    "    def get_experiment_results(self, experiment):\n",
    "        return measure_zero_shot_coordination(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vary_pm_experiment = VaryPMutateExperiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vary_pm_experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of experiment 0:\n",
      "Running experiment 1 (1/6 complete):\n",
      "Running experiment 0 (0/3 complete):\n",
      "Epoch 0, Time Taken (mm:ss): 0:34, Mean Loss: 3.847\n",
      "Test Loss: 4.314, Ground Truth F1-Score: 0.329, Student Error: 1.105, Teacher Error: 0.57, Protocol Diversity: 0.405,\n",
      "Epoch 1, Time Taken (mm:ss): 0:34, Mean Loss: 3.816\n",
      "Epoch 2, Time Taken (mm:ss): 0:34, Mean Loss: 3.867\n",
      "Epoch 3, Time Taken (mm:ss): 0:35, Mean Loss: 3.834\n"
     ]
    }
   ],
   "source": [
    "vary_pm_experiment.print_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'experiment': <__main__.MetaExperiment at 0x1fe98878a90>,\n",
       "  'status': 'Complete',\n",
       "  'results': [0.34375, 0.41875, 0.36250000000000004, 0.6, 0.2625, 0.26875],\n",
       "  'index': 0},\n",
       " {'experiment': <__main__.MetaExperiment at 0x1fe9896e630>,\n",
       "  'status': 'In Progress',\n",
       "  'results': None,\n",
       "  'index': 1},\n",
       " {'experiment': <__main__.MetaExperiment at 0x1fe98992080>,\n",
       "  'status': 'Not Run',\n",
       "  'results': None,\n",
       "  'index': 2},\n",
       " {'experiment': <__main__.MetaExperiment at 0x1fe9d47dc88>,\n",
       "  'status': 'Not Run',\n",
       "  'results': None,\n",
       "  'index': 3},\n",
       " {'experiment': <__main__.MetaExperiment at 0x1fe9d4a4860>,\n",
       "  'status': 'Not Run',\n",
       "  'results': None,\n",
       "  'index': 4},\n",
       " {'experiment': <__main__.MetaExperiment at 0x1fe9d4c6400>,\n",
       "  'status': 'Not Run',\n",
       "  'results': None,\n",
       "  'index': 5}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vary_pm_experiment.experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item['experiment'].p_mutate for item in vary_pm_experiment.experiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "EXPERIMENT_FOLDER = f'./experiments/vary_pm/{datetime.now().strftime(\"%d-%m_%H-%M\")}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./experiments/vary_pm/24-09_09-40'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPERIMENT_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved experiment data at: ./experiments/vary_pm/24-09_09-40/pm=0.0\n"
     ]
    }
   ],
   "source": [
    "for item_1 in vary_pm_experiment.experiments:\n",
    "    if item_1['status'] == 'Complete':\n",
    "        try:\n",
    "            meta_exp = item_1['experiment']\n",
    "            meta_exp_subfolder = \\\n",
    "                f'{EXPERIMENT_FOLDER}/pm={meta_exp.p_mutate}'\n",
    "            Path(meta_exp_subfolder).mkdir()\n",
    "            \n",
    "            meta_data = {\n",
    "                'index': item_1['index'], 'p_mutate': meta_exp.p_mutate\n",
    "            }\n",
    "            meta_data_path = Path(f'{meta_exp_subfolder}/meta.json')\n",
    "            with meta_data_path.open(mode='w') as f:\n",
    "                json.dump(meta_data, f)\n",
    "            \n",
    "            results = {\n",
    "                'zs_coord_f1_scores': item_1['results']\n",
    "            }\n",
    "            results_path = Path(f'{meta_exp_subfolder}/results.json')\n",
    "            with results_path.open(mode='w') as f:\n",
    "                json.dump(results, f)\n",
    "            \n",
    "            for item_2 in meta_exp.experiments:\n",
    "                logs_file_loc = \\\n",
    "                    f\"{meta_exp_subfolder}/experiment_{item_2['index']}_logs.json\"\n",
    "                logs_file_path = Path(logs_file_loc)\n",
    "                \n",
    "                with logs_file_path.open(mode='w') as f:\n",
    "                    json.dump(item_2['experiment'].training_history, f)\n",
    "            \n",
    "            print('Saved experiment data at:', meta_exp_subfolder)\n",
    "        except FileExistsError:\n",
    "            print(item_1['index'], 'Already Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_played, test_metrics = experiment.run_tests()\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_played, _ = experiment.run_tests()\n",
    "conf_matrix = compute_confusion_matrix(games_played)\n",
    "sns.heatmap(conf_matrix, annot=True, vmin=0, vmax=1)\n",
    "plt.title('Ground Truth Confusion Matrix')\n",
    "plt.ylabel('Predicted Class')\n",
    "plt.xlabel('Actual Class');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_played, _ = experiment2.run_tests()\n",
    "conf_matrix = compute_confusion_matrix(games_played)\n",
    "sns.heatmap(conf_matrix, annot=True, vmin=0, vmax=1)\n",
    "plt.title('Ground Truth Confusion Matrix')\n",
    "plt.ylabel('Predicted Class')\n",
    "plt.xlabel('Actual Class');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows correspond to the true labels and the columns to the predicted labels. Each column is divided by its sum in order to show the percentage of the time the model predicts the given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_played, _ = experiment.run_tests()\n",
    "mean_class_message_map = create_mean_class_message_map(games_played)\n",
    "sns.heatmap(mean_class_message_map, vmin=0, vmax=1);\n",
    "plt.ylabel('Class')\n",
    "plt.xlabel('Symbol')\n",
    "plt.title('Communication Protocol')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_played, _ = experiment2.run_tests()\n",
    "mean_class_message_map = create_mean_class_message_map(games_played)\n",
    "sns.heatmap(mean_class_message_map, vmin=0, vmax=1);\n",
    "plt.ylabel('Class')\n",
    "plt.xlabel('Symbol')\n",
    "plt.title('Communication Protocol')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_test_batch()\n",
    "outputs = play_game(\n",
    "    inputs, \n",
    "    experiment.teacher, \n",
    "    experiment2.student, \n",
    "    training=False, \n",
    "    p_mutate=0,\n",
    ")\n",
    "\n",
    "teacher_error, protocol_diversity = experiment.get_teacher_test_metrics([(inputs, targets, outputs)])\n",
    "f1, student_error = experiment.get_student_test_metrics([(inputs, targets, outputs)])\n",
    "loss = experiment.get_test_loss([(inputs, targets, outputs)])\n",
    "print(f'Teacher Error: {teacher_error}, Protocol Diversity {protocol_diversity}, Student Error: {student_error}, F1 Score {f1}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_game(inputs, outputs, targets, select_batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_test_batch()\n",
    "outputs = play_game(\n",
    "    inputs, \n",
    "    experiment2.teacher, \n",
    "    experiment.student, \n",
    "    training=False, \n",
    "    p_mutate=0,\n",
    ")\n",
    "\n",
    "teacher_error, protocol_diversity = experiment.get_teacher_test_metrics([(inputs, targets, outputs)])\n",
    "f1, student_error = experiment.get_student_test_metrics([(inputs, targets, outputs)])\n",
    "loss = experiment.get_test_loss([(inputs, targets, outputs)])\n",
    "print(f'Teacher Error: {teacher_error}, Protocol Diversity {protocol_diversity}, Student Error: {student_error}, F1 Score {f1}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_game(inputs, outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
